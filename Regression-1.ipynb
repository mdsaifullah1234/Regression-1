{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an \n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Simple linear regression involves predicting a target variable based on a single predictor variable, while multiple linear regression involves predicting the target variable based on two or more predictor variables. Example: Simple linear regression - predicting house prices based on square footage. Multiple linear regression - predicting house prices based on square footage, number of bedrooms, and location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in \n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = Assumptions of linear regression include linearity, independence, homoscedasticity, and normality of residuals. You can check these assumptions by examining residual plots, performing tests for linearity and normality, and checking for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using \n",
    "a real-world scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = The slope represents the change in the target variable for a one-unit change in the predictor variable, while the intercept represents the value of the target variable when all predictor variables are zero. Example: In a linear regression predicting exam scores based on study hours, the slope indicates how much an additional hour of study affects the exam score, and the intercept represents the expected score when no study hours are logged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Gradient descent is an optimization algorithm used to minimize the error of a model by adjusting its parameters iteratively. It works by calculating the gradient of the loss function with respect to the model parameters and updating the parameters in the direction that minimizes the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Multiple linear regression extends simple linear regression by allowing for multiple predictor variables to be included in the model equation. It can capture more complex relationships between the predictors and the target variable compared to simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and \n",
    "address this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = Multicollinearity occurs when predictor variables in a multiple linear regression model are highly correlated with each other. It can lead to unstable coefficient estimates and inflated standard errors. Detection methods include correlation matrices and variance inflation factors (VIFs). Addressing multicollinearity may involve removing variables, combining variables, or using regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = Polynomial regression is a type of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. Unlike linear regression, which assumes a linear relationship, polynomial regression can capture non-linear relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear \n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Advantages of polynomial regression include its ability to capture non-linear relationships and its flexibility in fitting complex data patterns. However, it can also suffer from overfitting, especially with higher degrees of polynomials, and may be computationally expensive. Polynomial regression is preferred when the relationship between variables is non-linear and cannot be adequately captured by linear regression."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
